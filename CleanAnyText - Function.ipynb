{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am trying to create a function to clean any given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"When access to digital computers became possible in the mid 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and as described below, each one developed its own style of research. John Haugeland named these symbolic approaches to AI \"good old fashioned AI\" or \"GOFAI\".[153] During the 1960s, symbolic approaches had achieved great success at simulating high-level \"thinking\" in small demonstration programs. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.[154] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAnyText(text):\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    punctuations= re.compile(r'[-.?!,:\"'';()|0-9]')\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"[\",\"the\")\n",
    "    text = text.replace(\"]\",\"the\")\n",
    "    text = text.replace(\"'\",\"the\")\n",
    "    text = text.replace(\"``\",\"the\")\n",
    "    text_tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "    text_tokens = [word for word in text_tokens if word not in stop_words]\n",
    "    post_punctuations = []\n",
    "    for words in text_tokens:\n",
    "        word = punctuations.sub(\"\",words)\n",
    "        if len(word)>0:\n",
    "            post_punctuations.append(word)\n",
    "    print(post_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'digital', 'computers', 'became', 'possible', 'mid', 's', 'ai', 'research', 'began', 'explore', 'possibility', 'human', 'intelligence', 'could', 'reduced', 'symbol', 'manipulation', 'research', 'centered', 'three', 'institutions', 'carnegie', 'mellon', 'university', 'stanford', 'mit', 'described', 'one', 'developed', 'style', 'research', 'john', 'haugeland', 'named', 'symbolic', 'approaches', 'ai', '``', 'good', 'old', 'fashioned', 'ai', \"''\", '``', 'gofai', \"''\", 'thethe', 's', 'symbolic', 'approaches', 'achieved', 'great', 'success', 'simulating', 'highlevel', '``', 'thinking', \"''\", 'small', 'demonstration', 'programs', 'approaches', 'based', 'cybernetics', 'artificial', 'neural', 'networks', 'abandoned', 'pushed', 'backgroundthethe', 'researchers', 's', 's', 'convinced', 'symbolic', 'approaches', 'would', 'eventually', 'succeed', 'creating', 'machine', 'artificial', 'general', 'intelligence', 'considered', 'goal', 'field']\n"
     ]
    }
   ],
   "source": [
    "cleanAnyText(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
